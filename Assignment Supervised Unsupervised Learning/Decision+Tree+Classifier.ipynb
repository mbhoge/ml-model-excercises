{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name = Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CONDA_PREFIX'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30185ffa4155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CONDA_PREFIX'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mr\"\\Library\\bin\\graphviz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[1;31m# raise KeyError with the original key value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CONDA_PREFIX'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r\"\\Library\\bin\\graphviz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Numpy </b>: Numpy is a package in python used for scientific calculating and perform various operations. Numpy Array is a          multidimensional array whose indexed is similar to Sequences which is start with Zero that is used to store values of          same datatype Numpy in python uses less memory to store data as compared with python list. Numpy provides multiple              functions they are where, nonzero and count_nonzero for finding the element.\n",
    "\n",
    "2. <b> Pandas </b>: Pandas is the most and favourite data science liabrary writtem for the python programming lanaguage for data manimulation and analysis also provides high_performance, easy to use strcutures and data analysis tools.\n",
    "\n",
    "3. <b>Matplotlib </b>: Matlpotlib which was introduced by John Hunter is a multi-platform data visualization library built on Numpy arrays and designed to work with the broader SciPY stack. Similarly, Maltplot.pyplot is a collection of command style functions that make matplotlib work like MATLAB. Pyplot is mainly intented for interactive plots and simple cases of programmatic plot generation.\n",
    "\n",
    "4. <b>Seaborn</b>: Seaborn is a python data data visualization library based on matplotlib which provides high-level interfrace for drawing atractive and informative statistical graph. \n",
    "\n",
    "<b>Matplotlib vs Seaborn</b> <br>\n",
    "Matplotlib is mainly used for basic plotting. Visulization using matplotlib generally consists of bars, pies, lines,scatter plot while seaborn provides variety of visualization patterns which use less syntax and has best themes.\n",
    "\n",
    "6. <b> OS </b>: The OS module also known as Python's standard utility modules which provides functions for interacting with the operating system. This module is implemented by calling the standard C function system() method and has the same limitations.\n",
    "\n",
    "7. <b> os.environ </b> : os.environ in python is a mapping object that represents the user's environment variables. This os.environ behaves like python dictionary, so all the common dictionary operations like get and set can be performed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "features , labels = make_classification(n_samples = 500, n_features = 4 , n_informative=3, \n",
    "                                            n_classes = 3 , weights=[0.2, 0.3, 0.5] ,n_redundant=0, n_clusters_per_class = 1 , random_state =584)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> sklearn </b>:  sklearn is machine learning library for Python that provides simple and efficient tools for data  analysis and data mining which is accessible to everybody and reusable in various context. It is build on <b> NumPy </b> and <b> SciPy </b> which is open source and commercially usable (BSD license) \n",
    "\n",
    "<b> sklearn.datasets </b> In this decision tree classification problem based on supervised machine learning I have used sklearn package to generate some random n-class classification problem.\n",
    "\n",
    "<b>List of following parameters which I make used to generate random n-class classification problem are: </b> <br>\n",
    "a. <b> n_samples </b> : First parameter n_samples means the number of samples which have implicit datatype (i.e. int). By default n_samples store 100 of data. In my Case I have stored 500 samples of data \n",
    "\n",
    "b. <b> n_features </b>: Second parameter n_features is the total number of features which determined how many columns of features the generated dataset will have. In Machine learning, features correspond to the numerical characteristics data. For example, in the above parameter n_features is set to int 4 i.e. 'Features_01' , 'Features_02' , 'Features_03' , 'Features_04'. so, there are 4 numerical columns in the random generated dataset. By adding more features on make_classification hence complexity of random dataset.\n",
    "\n",
    "c. <b> n_informative </b>: Third parameter n_informative is the number of informative features. In machine learning informative correspond to the numerical value data. For example, in the above parameter n_informarive is to int 3 which is composed of a number of guassian clusters each located around the vertices of a hypercube in a subspace of dimension n_informative. <br>\n",
    "    - Guassian Cluster : Gaussian Mixtire Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of there dstributions represent a cluster. \n",
    "\n",
    "d. <b> n_classes </b>: Fourth Parameter i.e. n_classes is the number or claaes or labels of the classification problem. For example, in above parameter n_classes is set to 3 which means it is highly dependent to n_features.\n",
    "\n",
    "e. <b> weights </b>: Fifth parameter weights is same as list. In our case sum of weights is equal to 1 which returned n_samples.\n",
    "\n",
    "f. <b> n_redundant </b>: n_redundant is the number of the reduntant features which generated random linear combinations of the informative features.\n",
    "\n",
    "g. <b> n_clusters_per_class </b>: n_clusters_per_class is the number of clusters per class\n",
    "\n",
    "h. <b> random_state </b>:  If you do not specify the <b> random_state </b> in your code, then every time you run(execute) your code a new random value is gnerated and the train and test datasets would have different values each time.\n",
    "\n",
    "However, if a fixed value is assigned like <b>random_state </b> = 584 then no matter how many times you execute your code the result would be the same i.e. sama values in train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> feature.shape </b>: The feature.shape property is usually used to get the current shape of an feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> labels.shape </b>: The labels.shape property is usually used to get the current shape of an labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0] # returns feature value at 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[2] # returns label value at 2 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of total features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = ['Features_01' , 'Features_02' , 'Features_03' , 'Features_04']\n",
    "features_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### pd.dataframe is two-dimensional data structure where data is aligned in a tabular structured of row and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(features, columns = features_name)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name = ['Outcome']\n",
    "labels_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(labels , columns = labels_name)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> pd.concat </b>: Pandas provide various facilities for easily combining different dataframe together. In our case, we have joined features dataframe and label dataframe where feature dataframe is independent variable and label dataframe is dependent with feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([features_df , labels_df], axis =1 , join = 'inner' , sort = False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe method()\n",
    "    - describe method () is used to analyzes both numeric and object series and also the DataFrame column sets of mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counting of outcome\n",
    "\n",
    "<b> Pandas value_counts </b> built pandas function that returns an object containing counts of unique value in sorted order. In our case, I have used value_counts function and plotted histogram of outcome based on classification of outcome i.e 0 , 1  and 2\n",
    "\n",
    "1. <b> plt.title </b>: Assigning title using  plt.title\n",
    "2. <b> plt.xlabel </b>  and <b> plt.ylabel </b> we can assign labels to those respective axis. \n",
    "3. <b> plt.grid </b> is used to add grid lines changes in axix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_counts = pd.value_counts(dataset['Outcome'] , sort = True).sort_index()\n",
    "outcome_counts.plot(kind = 'bar' , color = 'Gray')\n",
    "plt.title('Histogram of Outcome', fontweight='bold', fontsize = '15', color = \"gray\")\n",
    "plt.xlabel('Outcome', fontweight='bold', fontsize = '15', color = \"gray\")\n",
    "plt.ylabel('Features', fontweight='bold', fontsize = '15', color = \"gray\")\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info method()\n",
    "    - pandas info () function is used to get a concise summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_outcome = dataset[dataset['Outcome'] == 0]\n",
    "medium_outcome = dataset[dataset['Outcome'] == 1]\n",
    "high_outcome = dataset[dataset['Outcome'] == 2]\n",
    "print('Low outcome of our dataset    :', low_outcome.shape)\n",
    "print('Middle outcome of our dataset :', medium_outcome.shape)\n",
    "print('High outcome of our dataset   :', high_outcome.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> sns.scatterplot </b>: sns.scatterplot is used to show the relationship between x and y for different subset of the data using the hue, style and  palette paameters which is used for making graphics more accessible.\n",
    "\n",
    "1. <b> x and y </b>: x and y is the input data variables which can be directly or reference column in dataset.\n",
    "\n",
    "2. <b> hue  </b>:  name of variables that is used for grouping variables which produce points with different colors. For example, In our case class 0, 1 and 2 is different class separated by different colors.\n",
    "\n",
    "3. <b> style </b>: style is the name of variable that will produce points with different markers.\n",
    "\n",
    "4. <b> palatte </b>: Colors to use for the different lavels of the hue variable.\n",
    "\n",
    "5. <b> data </b>: Dataframe where each column is a variable and each row is observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_01' , y ='Features_02' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_01' , y ='Features_03' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_01' , y ='Features_04' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_02' , y ='Features_03' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_02' , y ='Features_04' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Features_03' , y ='Features_04' , hue = 'Outcome'  , style = 'Outcome' , palette='viridis', data = dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> scipy.stats </b>: This module contains large number of probability distributions as well as a growing library of statistical functions.\n",
    "\n",
    "<b> scipy.stats.norm </b> : It is used for normal continous variable\n",
    "\n",
    "<b> matplotlib.gridspec </b>: I have used a gridspec module to specifies the location of the subplot in the figure also this module specifies the geometry of the grid that a subplot will be placed.\n",
    "\n",
    "<b> gridspec.GridSpec(28,1) </b>: Helps to set number of rows and number of columns.\n",
    "\n",
    "\n",
    "I have choose bins equal to 50 then input will be divided into 50 intervals or bins if possible.\n",
    "\n",
    "I have used loop to specifies geometry of grid from Features_01 to Features_04\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import matplotlib.gridspec as gridspec\n",
    "col_features = dataset.iloc[:,0:4].columns\n",
    "# col_features\n",
    "bins = 50 \n",
    "\n",
    "plt.figure( figsize = (12, 28 * 4))\n",
    "gs = gridspec.GridSpec(28,1) \n",
    "\n",
    "for count, elem in enumerate(dataset[col_features]):\n",
    "    ax = plt.subplot(gs[count])\n",
    "    sns.distplot(dataset[elem][dataset.Outcome == 0], bins = bins, fit=norm )\n",
    "    sns.distplot(dataset[elem][dataset.Outcome == 1], bins = bins, fit=norm)\n",
    "    sns.distplot(dataset[elem][dataset.Outcome == 2], bins = bins, fit=norm)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(elem), fontsize = '15', color = \"gray\" )\n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> SMOTE </b>  is an oversampling method which creates synthetic examples from minor class instead of creating samples. The algorithm selects two or more similar instances and creates one attribute instance at a time by a random amount within the difference to the neighboring instances.\n",
    "\n",
    "SMOTE Process <br>\n",
    "    - Indentify the feature vector and its nearest neighbors\n",
    "    - Take the difference betweeen two neighbors between vector\n",
    "    - Multiply the difference with a random number between 0 and  1\n",
    "    - Identify a new point on the line segment by adding random number to feature vector \n",
    "    - Repeat the process for identified features vectors.\n",
    "\n",
    "SMOTE method is approached to increase the minority classes in random created data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE as sm\n",
    "x_sampled , y_sampled = sm().fit_resample(features, labels)\n",
    "from collections import Counter ## separate collection for 0, 1 and 2 \n",
    "print(sorted(Counter(y_sampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Sklearn </b> provides best function for partitioning data into training set and testing set. We provide certain proportion of data to use as a test set and we can provide the parameter random_state to ensure repeatable results. test_size parameter decides the size of the data that has to be split as the the test dataset. In our we have used 20% of data for our testset and rest of 80% for train test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain , xTest , yTrain , yTest = train_test_split(x_sampled , y_sampled , test_size = 0.2 , random_state = 584)\n",
    "print('Total training features :', xTrain.shape)\n",
    "print('Total testing outcomes :', xTest.shape)\n",
    "\n",
    "x_train_sampled_data = pd.DataFrame(xTrain)\n",
    "y_train_sampled_data = pd.DataFrame(yTrain)\n",
    "x_test_sampled_data  = pd.DataFrame(xTest)\n",
    "y_test_sampled_data  = pd.DataFrame(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Decision Tree </b>: A decision tree also known as classification and predition problem based on supervised machine learning where data is continuoulsy split according to the certain parameter.\n",
    "\n",
    "#### Decision Tree consist of: \n",
    "\n",
    "1. <b> Nodes </b>: Test for the value of a certain attribute.\n",
    "\n",
    "2. <b> Edges/ Branch </b>:  Correspond to the outcome of a test and connect to the next node or leaf.\n",
    "\n",
    "3. <b> Leaf Nodes </b> : Terminal nodes that predict the outcome\n",
    "\n",
    "### Decision Tree Classifier \n",
    "\n",
    "a. Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG) (reduction in uncertainty towards the final decision).\n",
    "\n",
    "b. In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each leaf node all belong to the same class. <br>\n",
    "\n",
    "c. In practice, we may set a limit on the depth of the tree to prevent overfitting. We compromise on purity here somewhat as the final leaves may still have some impurity.\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "a. Cross validation is a resampling procecure used to evaluate machine learning models on a limeted data sample.\n",
    "\n",
    "b. Technique that involves partitioning the original observation dataset into:\n",
    "    i. training set - used to train the model, \n",
    "    ii. an independent set -  used to evaluate the analysis. <br>\n",
    "c. The most common cross-validation technique is k-fold cross-validation in our solution I have put the value of K = 10 which means dataset will run 10 times and below the process will run 5 times, each time with different outout set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "clf = clf.fit(x_train_sampled_data,y_train_sampled_data)\n",
    "yPred = clf.predict(x_test_sampled_data)\n",
    "scores = cross_val_score(clf, y_test_sampled_data, yPred, cv=10)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(yTest, yPred))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b> confusion matrix </b> is an N by N matrix where N is the number of classes being predicted. Confusion matrix provide a more detailed breakdown of correct and incorrect classifications for each class where daigonal elements represents the number of points for which the predicted label is equal to the true label while anything off the diagonal was mislabeled by the classifier.\n",
    "For best evaluation we need higher value in the diagonal of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n",
    "y_predicted =  np.array(clf.predict(x_test_sampled_data))\n",
    "y_right = np.array(y_test_sampled_data)\n",
    "\n",
    "cnf = confusion_matrix(y_right, y_predicted )\n",
    "np.set_printoptions(precision=2)\n",
    "cnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Accuracy </b> is the mostly common used matrix to evaulated the model which is actually not a clear indicator of the performance.\n",
    "\n",
    "Accuracy = (34 + 52 + 33) / (34 + 13 + 5 + 52 + 13 + 13) = 0.793\n",
    "\n",
    "</b> Precision </b> is the percentage of positive instances out of the total predicted positive instances which means precision or positive predicted value means how much model is right when it says it is right.\n",
    "\n",
    "Precision of Class 0 = (34/47) = 0.72 <br>\n",
    "Precision of Class 1 = (52/65) = 0.8 <br>\n",
    "Precision of classs 2 = (33/38) = 0.86 <br>\n",
    "\n",
    "<b> Recall </b> is the percentage of positive instances out of the total actual positive instances. Recall shows how much extra right ones, the model missed when os showed the right ones.\n",
    "\n",
    "Recall of class 0 = 34 / (52) = 0.65 <br>\n",
    "Recall of class 1 = 52/ 52 = 1 <br>\n",
    "Recall of class 2 = 33/46 = 0.717 <br>\n",
    "\n",
    "<b> F1-Score</b>: F1- score is the harmoncic mean of the precision and recall which means higher the value of f1-score better will be the model. due to the product in the numerator if one goes low, the final F1 score goes down significantly. So a model does well in F1 score if the positive predicted are actually positives (precision) and doesn't miss out on positives and predicts them negative (recall).\n",
    "\n",
    "F1-Score of class 0: 2PR/(P + R) =2 * 0.72 * 0.65 /(0.72 + 0.65) = 0.69 <br> \n",
    "F1-Score of class 1 = 2 * 0.8 * 1 / (0.8 + 1) = 0.89 <br>\n",
    "F1 - Score of class 2 = 2 * 0.86 * 0.717 / (0.86 + 0.717) = 0.79 <br>\n",
    "\n",
    "<b> Support </b> : It is the total number of element in each predicted class. Here, support for class 0 , 1 and 2 are 52, 52 and 46 respectively\n",
    "\n",
    "<b> Macro Average </b>  = It is the normal average \n",
    "\n",
    "<b> Micro Average </b>  = sum(correct classification) / Total samples\n",
    "\n",
    "<b> Weighted Average </b> = (Precision * Total Predicted) / Total Sample\n",
    "\n",
    "<b> Specificity </b> also known as true negative class which is the number of items correctly identified as negative out of the total negatives. Exact oppositive to Recall\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_right, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cnf , annot =  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above used heatmap is a graphical representation of data in which heat maps displays numeric tabular data where the cells are colored depending upon the contained value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(\n",
    "    clf, out_file=None, feature_names=features_name)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('decision_tree.png')\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is used to plot the Decision tree. We can also export the tree in Graphviz format using the export_graphviz function exporter. The export_graphviz exporter also supports a variety oad aesthetic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
